{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path_prefix = \"../\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(path_prefix))\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Special Imports\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Activation, Dropout\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Project Imports\n",
    "from config import path_config\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define required Classes and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Local Response Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalResponseNormalization(Layer):\n",
    "\n",
    "    def __init__(self, n = 5, alpha = 0.0005, beta = 0.75, k = 2, **kwargs):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        super(LocalResponseNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        super(LocalResponseNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        _, r, c, f = self.shape\n",
    "        squared = K.square(x)\n",
    "        pooled = K.pool2d(squared, (self.n, self.n), strides=(1, 1), padding=\"same\", pool_mode=\"avg\")\n",
    "\n",
    "        summed = K.sum(pooled, axis=3, keepdims=True)\n",
    "        averaged = self.alpha * K.repeat_elements(summed, f, axis=3)\n",
    "\n",
    "        denom = K.pow(self.k + averaged, self.beta)\n",
    "        return x / denom\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Wrapper class for selecting Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModel:\n",
    "    \"\"\"\n",
    "    Wrapper class for Model Selection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, height, width, channels):\n",
    "        \"\"\"\n",
    "\n",
    "        :param height:\n",
    "        :param width:\n",
    "        :param channels:\n",
    "        \"\"\"\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        model = None\n",
    "\n",
    "        if path_config.MODEL_NAME == 'MAlexNet':\n",
    "            model = MAlexNetModel(self.height, self.width, self.channels)\n",
    "        elif path_config.MODEL_NAME == 'AlexNet':\n",
    "            model = AlexNetModel(self.height, self.width, self.channels)\n",
    "        elif path_config.MODEL_NAME == 'InceptionV4':\n",
    "            model = InceptionV4(self.height, self.width, self.channels)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. M-AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAlexNetModel:\n",
    "    \"\"\"\n",
    "    mAlexNet model in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, height, width, channels):\n",
    "        \"\"\"\n",
    "\n",
    "        :param height:\n",
    "        :param width:\n",
    "        :param channels:\n",
    "        \"\"\"\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Conv2D(16, (11, 11), padding='same', strides=(4, 4), input_shape=(self.height, self.width, self.channels),\n",
    "                   name='mAlex_conv1'))\n",
    "        model.add(Activation('relu', name='mAlex_relu1'))\n",
    "        model.add(LocalResponseNormalization(name='mAlex_norm1'))\n",
    "        model.add(MaxPooling2D(pool_size=3, strides=2, padding='same', name='mAlex_pool1'))\n",
    "\n",
    "        model.add(Conv2D(20, (5, 5), padding='same', strides=(1, 1), name='mAlex_conv2'))\n",
    "        model.add(Activation('relu', name='mAlex_relu2'))\n",
    "        model.add(LocalResponseNormalization(name='mAlex_norm2'))\n",
    "        model.add(MaxPooling2D(pool_size=3, strides=2, padding='same', name='mAlex_pool2'))\n",
    "\n",
    "        model.add(Conv2D(30, (3, 3), padding='same', strides=(1, 1), name='mAlex_conv3'))\n",
    "        model.add(Activation('relu', name='mAlex_relu3'))\n",
    "        model.add(MaxPooling2D(pool_size=3, strides=2, padding='same', name='mAlex_pool3'))\n",
    "\n",
    "        model.add(Flatten(name='mAlex_flatten'))\n",
    "\n",
    "        model.add(Dense(48, activation='relu', name='mAlex_fc4'))\n",
    "        model.add(Dense(1, activation='sigmoid', name='mAlex_fc5'))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetModel:\n",
    "    \"\"\"\n",
    "    AlexNet model in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, height, width, channels):\n",
    "        \"\"\"\n",
    "\n",
    "        :param height:\n",
    "        :param width:\n",
    "        :param channels:\n",
    "        \"\"\"\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Conv2D(96, (11, 11), padding='same', strides=(4, 4), input_shape=(self.height, self.width, self.channels),\n",
    "                   name='Alex_conv1'))\n",
    "        model.add(Activation('relu', name='Alex_relu1'))\n",
    "        model.add(MaxPooling2D(pool_size=3, strides=2, padding='same', name='Alex_pool1'))\n",
    "        model.add(LocalResponseNormalization(name='Alex_norm1'))\n",
    "\n",
    "        model.add(Conv2D(256, (5, 5), padding='same', strides=(1, 1), name='Alex_conv2'))\n",
    "        model.add(Activation('relu', name='Alex_relu2'))\n",
    "        model.add(MaxPooling2D(pool_size=3, strides=2, padding='same', name='Alex_pool2'))\n",
    "        model.add(LocalResponseNormalization(name='Alex_norm2'))\n",
    "\n",
    "        model.add(Conv2D(384, (3, 3), padding='same', strides=(1, 1), name='Alex_conv3'))\n",
    "        model.add(Activation('relu', name='Alex_relu3'))\n",
    "\n",
    "        model.add(Conv2D(384, (3, 3), padding='same', strides=(1, 1), name='Alex_conv4'))\n",
    "        model.add(Activation('relu', name='Alex_relu4'))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same', strides=(1, 1), name='Alex_conv5'))\n",
    "        model.add(Activation('relu', name='Alex_relu5'))\n",
    "        model.add(MaxPooling2D(pool_size=3, strides=2, padding='same', name='Alex_pool5'))\n",
    "        model.add(Dropout(rate=0.5, name='Alex_dropout5'))\n",
    "\n",
    "        model.add(Flatten(name='Alex_flatten'))\n",
    "\n",
    "        model.add(Dense(units=4096, activation='relu', name='Alex_fc6'))\n",
    "        model.add(Dropout(rate=0.5, name='Alex_dropout6'))\n",
    "\n",
    "        model.add(Dense(units=4096, activation='relu', name='Alex_fc7'))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid', name='mAlex_fc8'))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. InceptionV4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV4:\n",
    "    \"\"\"\n",
    "        Inception v4 model in Keras\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, height, width, channels):\n",
    "        \"\"\"\n",
    "\n",
    "        :param height:\n",
    "        :param width:\n",
    "        :param channels:\n",
    "        \"\"\"\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_generator(train_type, path_prefix):\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    model_obj = None\n",
    "    \n",
    "    # Import model graph\n",
    "    if train_type == 'rgb':\n",
    "        model_obj = WrapperModel(path_config.IMG_HEIGHT, path_config.IMG_WIDTH, path_config.N_CHANNELS)\n",
    "    elif train_type == 'grayscale':\n",
    "        model_obj = WrapperModel(path_config.IMG_HEIGHT, path_config.IMG_WIDTH, 1)\n",
    "    \n",
    "    # Create instance of DL Model\n",
    "    model = model_obj.get_model()\n",
    "    \n",
    "    # Build the DL Model\n",
    "    model = model.build_model()\n",
    "    \n",
    "    # Pring DL Model Summary\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Create instance of Image Data Generator\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        rescale=1. / 255,\n",
    "        validation_split=0.20\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        path_prefix + path_config.PATH_DATA_PROCESSED + path_config.PATH_DATA_COMBINED,\n",
    "        subset='training',\n",
    "        target_size=(path_config.IMG_HEIGHT, path_config.IMG_WIDTH),\n",
    "        color_mode='rgb',\n",
    "        batch_size=path_config.BATCH_SIZE_TRAIN,\n",
    "        class_mode='binary',\n",
    "        classes=['empty', 'occupied'],\n",
    "        # save_to_dir = '../../Datasets/Augmented/training/',\n",
    "        # save_format='jpeg',\n",
    "        seed=100)\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        path_prefix + path_config.PATH_DATA_PROCESSED + path_config.PATH_DATA_COMBINED,\n",
    "        subset='validation',\n",
    "        target_size=(path_config.IMG_HEIGHT, path_config.IMG_WIDTH),\n",
    "        color_mode='rgb',\n",
    "        batch_size=path_config.BATCH_SIZE_VALIDATION,\n",
    "        class_mode='binary',\n",
    "        classes=['empty', 'occupied'],\n",
    "        # save_to_dir='../../Datasets/Augmented/validation/',\n",
    "        # save_format='jpeg',\n",
    "        seed=100)\n",
    "\n",
    "    # Optimizer\n",
    "    model_sgd = SGD(lr=path_config.LEARNING_RATE, \n",
    "                    decay=path_config.WEIGHT_DECAY_RATE, \n",
    "                    momentum=path_config.MOMENTUM,\n",
    "                    nesterov=path_config.FLAG_NESTEROV)\n",
    "\n",
    "    # Callbacks\n",
    "    model_tb = TensorBoard(\n",
    "        log_dir=path_prefix + path_config.PATH_LOGS_TENSORBOARD + datetime.datetime.now().strftime(\n",
    "            '%Y-%m-%d_%H-%M-%S') + '_epoch_' + str(\n",
    "            path_config.EPOCHS) + '_batch_' + str(path_config.BATCH_SIZE_TRAIN),\n",
    "        histogram_freq=0, write_graph=True, write_images=True)\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath=path_prefix + path_config.PATH_WEIGHTS + path_config.WEIGHTS_NAME + '-checkpoint_' + start_time.strftime(\n",
    "            '%Y-%m-%d_%H-%M') + '_' + '{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        verbose=1)\n",
    "    \n",
    "    print('Model training begins...')\n",
    "    \n",
    "    # Training model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=model_sgd, metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=path_config.STEPS_TRAIN,\n",
    "        epochs=path_config.EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=path_config.STEPS_VALIDATION,\n",
    "        callbacks=[model_checkpoint, model_tb])\n",
    "\n",
    "    print('Model training completed.')\n",
    "    \n",
    "    model_weight_filename = path_prefix + path_config.PATH_MODEL + path_config.MODEL_NAME + '_' + train_type + '_generator_' + start_time.strftime(\n",
    "        '%Y-%m-%d_%H-%M') + '.h5'\n",
    "\n",
    "    print('Saving trained model weights...')\n",
    "    model.save_weights(model_weight_filename)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    print('Total Time taken to train the model: ' + str((end_time - start_time).total_seconds() / 60.0))\n",
    "\n",
    "    return model_weight_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_comparison(origin_image, transformed_image):\n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.subplot(121), plt.imshow(origin_image, cmap='gray')\n",
    "    plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122), plt.imshow(transformed_image, cmap='gray')\n",
    "    plt.title('Transformed Image'), plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "class ModelPredict:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        model_obj = WrapperModel(path_config.IMG_HEIGHT, path_config.IMG_WIDTH, path_config.N_CHANNELS)\n",
    "        model = model_obj.get_model()\n",
    "        model = model.build_model()\n",
    "\n",
    "        gray_model_obj = WrapperModel(path_config.IMG_HEIGHT, path_config.IMG_WIDTH, 1)\n",
    "        gray_model = gray_model_obj.get_model()\n",
    "        gray_model = gray_model.build_model()\n",
    "\n",
    "        self.model = model\n",
    "        self.gray_model = gray_model\n",
    "\n",
    "    def model_predict_rgb(self, model_weight_filepath):\n",
    "        \"\"\"\n",
    "\n",
    "        :param model_weight_filepath:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        model.load_weights(model_weight_filepath)\n",
    "        sample_image = path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000001.jpg'\n",
    "        im = cv2.imread(sample_image)\n",
    "\n",
    "        x_shift = 130\n",
    "        y_shift = 300\n",
    "        x_range = [100, 300, 550, 750]\n",
    "        y_range = [100, 600]\n",
    "\n",
    "        im = cv2.resize(im, (1000, 1000))\n",
    "        im = np.array(im)\n",
    "\n",
    "        images = []\n",
    "        for x in x_range:\n",
    "            for y in y_range:\n",
    "                im_ = im[x + 2:x + x_shift - 2, y + 2:y + y_shift - 2]\n",
    "                im_ = cv2.resize(im_, (path_config.IMG_WIDTH, path_config.IMG_HEIGHT))\n",
    "                im_ = np.array(im_)\n",
    "                # im_ = im_.transpose(1, 0, 2)\n",
    "                images.append(im_)\n",
    "\n",
    "        images = np.array(images)\n",
    "\n",
    "        predictions = model.predict(images)\n",
    "        predictions = np.hstack(predictions > path_config.ACCURACY_THRESHOLD).astype(int)\n",
    "\n",
    "        i = 0\n",
    "        im_ = np.copy(im)\n",
    "        for x in x_range:\n",
    "            for y in y_range:\n",
    "                im_ = cv2.rectangle(im_, (y, x + 2), (y + y_shift, x + x_shift - 2),\n",
    "                                    (255 * int(predictions[i]), 255 * int(1 - predictions[i]), 0), 2)\n",
    "                i += 1\n",
    "\n",
    "        show_comparison(im, im_)\n",
    "\n",
    "    def crop_and_rotate(self, img, cnt, rect, box):\n",
    "        \"\"\"\n",
    "\n",
    "        :param img:\n",
    "        :param cnt:\n",
    "        :param rect:\n",
    "        :param box:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        W = rect[1][0]\n",
    "        H = rect[1][1]\n",
    "\n",
    "        Xs = [i[0] for i in box]\n",
    "        Ys = [i[1] for i in box]\n",
    "        x1 = min(Xs)\n",
    "        x2 = max(Xs)\n",
    "        y1 = min(Ys)\n",
    "        y2 = max(Ys)\n",
    "\n",
    "        angle = rect[2]\n",
    "\n",
    "        if W >= H:\n",
    "            angle = angle + 90\n",
    "            croppedW = H\n",
    "            croppedH = W\n",
    "        else:\n",
    "            croppedW = W\n",
    "            croppedH = H\n",
    "\n",
    "        # Center of rectangle in source image\n",
    "        center = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "\n",
    "        '''\n",
    "        # Size of the upright rectangle bounding the rotated rectangle\n",
    "        size = (x2 - x1, y2 - y1)\n",
    "        M = cv2.getRotationMatrix2D((size[0] / 2, size[1] / 2), angle, 1.0)\n",
    "    \n",
    "        # Cropped upright rectangle\n",
    "        cropped = cv2.getRectSubPix(img, size, center)\n",
    "        cropped = cv2.warpAffine(cropped, M, size, borderMode = cv2.BORDER_CONSTANT)\n",
    "    \n",
    "        # Final cropped & rotated rectangle\n",
    "        croppedRotated = cv2.getRectSubPix(cropped, (int(croppedW), int(croppedH)), (size[0] / 2, size[1] / 2))\n",
    "        '''\n",
    "\n",
    "        cnt = np.array([cnt[0], cnt[1], cnt[3], cnt[2]], np.float32)\n",
    "        window = np.array([[0, 0], [croppedW, 0], [0, croppedH], [croppedW, croppedH]], np.float32)\n",
    "        M = cv2.getPerspectiveTransform(cnt, window)\n",
    "        cropped = cv2.warpPerspective(img, M, (int(croppedW), int(croppedH)))\n",
    "\n",
    "        return cropped\n",
    "\n",
    "    def model_predict_rgb_xml(self, model_weight_filepath, sample_image_path, sample_image_xml_path, thickness=2,\n",
    "                              grayscale_flag=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param model_weight_filepath:\n",
    "        :param sample_image_path:\n",
    "        :param sample_image_xml_path:\n",
    "        :param thickness:\n",
    "        :param grayscale_flag:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if grayscale_flag:\n",
    "            model = self.gray_model\n",
    "        else:\n",
    "            model = self.model\n",
    "        model.load_weights(model_weight_filepath)\n",
    "\n",
    "        pklot_pupcr_xml_tree = ET.parse(sample_image_xml_path)\n",
    "        pklot_pupcr_xml_root = pklot_pupcr_xml_tree.getroot()\n",
    "\n",
    "        im = cv2.imread(sample_image_path)\n",
    "        im = np.array(im)\n",
    "        image_name = (sample_image_path.split('.')[-2]).split('/')[-1]\n",
    "\n",
    "        contour_display_image = im.copy()\n",
    "\n",
    "        for space in pklot_pupcr_xml_root:\n",
    "            for value in space:\n",
    "                if value.tag == 'contour':\n",
    "                    cnt = []\n",
    "                    for contour in value:\n",
    "                        cnt.append([[int(contour.attrib['x']), int(contour.attrib['y'])]])\n",
    "\n",
    "                    cnt = np.array(cnt)\n",
    "                    rect = cv2.minAreaRect(cnt)\n",
    "                    # print(rect)\n",
    "                    box = cv2.boxPoints(rect)\n",
    "                    box = np.int0(box)\n",
    "                    # print(box)\n",
    "                    im_ = self.crop_and_rotate(im, cnt, rect, box)\n",
    "                    im_ = cv2.resize(im_, (path_config.IMG_WIDTH, path_config.IMG_HEIGHT))\n",
    "                    im_ = np.array(im_)\n",
    "                    cv2.imwrite(path_prefix + path_config.PATH_DATA_TRANSFORMED + image_name + str(space.attrib.get('id')) + '.jpg',\n",
    "                                im_)\n",
    "\n",
    "                    im_ = im_ / 255.0\n",
    "                    # show_comparison(im, im_)\n",
    "\n",
    "                    if grayscale_flag:\n",
    "                        im_ = get_grayscale(im_)\n",
    "                        im_ = apply_gaussian_mask(im_)\n",
    "                        im_ = get_fft(im_)\n",
    "                        im_ = cv2.normalize(im_, None, 0, 1, cv2.NORM_MINMAX)\n",
    "                        im_ = np.expand_dims(im_, axis=2)\n",
    "\n",
    "                    im_ = np.expand_dims(im_, axis=0)\n",
    "                    prediction = model.predict(im_)\n",
    "                    # print('ORIGINAL: ', prediction)\n",
    "\n",
    "                    if prediction >= path_config.ACCURACY_THRESHOLD:\n",
    "                        prediction = 1\n",
    "                    else:\n",
    "                        prediction = 0\n",
    "\n",
    "                    # print('MODIFIED: ', prediction)\n",
    "                    # print('\\n')\n",
    "\n",
    "                    cv2.drawContours(contour_display_image, [cnt], 0,\n",
    "                                     (0, 255 * int(1 - prediction), 255 * int(prediction)),\n",
    "                                     thickness)\n",
    "\n",
    "        cv2.imwrite(path_prefix + path_config.PATH_DATA_VALIDATION_RESULTS + image_name + '_res' + '.jpg',\n",
    "                    contour_display_image)\n",
    "\n",
    "        plt.imshow(contour_display_image, cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "\n",
    "    def model_predict_ensemble(self, model_weight_filepath_rgb, model_weight_filepath_grayscale, sample_image_path,\n",
    "                               sample_image_xml_path, thickness=2):\n",
    "        \"\"\"\n",
    "\n",
    "        :param model_weight_filepath_rgb:\n",
    "        :param model_weight_filepath_grayscale:\n",
    "        :param sample_image_path:\n",
    "        :param sample_image_xml_path:\n",
    "        :param thickness:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        model1 = self.model\n",
    "        model2 = self.gray_model\n",
    "\n",
    "        model1.load_weights(model_weight_filepath_rgb)\n",
    "        model2.load_weights(model_weight_filepath_grayscale)\n",
    "\n",
    "        pklot_pupcr_xml_tree = ET.parse(sample_image_xml_path)\n",
    "        pklot_pupcr_xml_root = pklot_pupcr_xml_tree.getroot()\n",
    "\n",
    "        im = cv2.imread(sample_image_path)\n",
    "        im = np.array(im)\n",
    "\n",
    "        contour_display_image = im.copy()\n",
    "\n",
    "        for space in pklot_pupcr_xml_root:\n",
    "            for value in space:\n",
    "                if value.tag == 'contour':\n",
    "                    cnt = []\n",
    "                    for contour in value:\n",
    "                        cnt.append([[int(contour.attrib['x']), int(contour.attrib['y'])]])\n",
    "\n",
    "                    cnt = np.array(cnt)\n",
    "                    rect = cv2.minAreaRect(cnt)\n",
    "                    # print(rect)\n",
    "                    box = cv2.boxPoints(rect)\n",
    "                    box = np.int0(box)\n",
    "                    # print(box)\n",
    "                    im_ = self.crop_and_rotate(im, cnt, rect, box)\n",
    "                    im_ = cv2.resize(im_, (path_config.IMG_WIDTH, path_config.IMG_HEIGHT))\n",
    "                    im_ = np.array(im_)\n",
    "\n",
    "                    image_rgb = np.expand_dims(im_, axis=0)\n",
    "                    prediction_rgb = model1.predict(image_rgb)\n",
    "\n",
    "                    im_ = get_grayscale(im_)\n",
    "                    im_ = apply_gaussian_mask(im_)\n",
    "                    im_ = get_fft(im_)\n",
    "                    im_ = cv2.normalize(im_, None, 0, 1, cv2.NORM_MINMAX)\n",
    "                    im_ = np.expand_dims(im_, axis=2)\n",
    "                    image_grayscale = np.expand_dims(im_, axis=0)\n",
    "                    prediction_grayscale = model2.predict(image_grayscale)\n",
    "\n",
    "                    prediction = max([prediction_rgb, prediction_grayscale])\n",
    "\n",
    "                    if prediction >= path_config.ACCURACY_THRESHOLD:\n",
    "                        prediction = 1\n",
    "                    else:\n",
    "                        prediction = 0\n",
    "\n",
    "                    cv2.drawContours(contour_display_image, [box], 0,\n",
    "                                     (255 * int(prediction), 255 * int(1 - prediction), 0),\n",
    "                                     thickness)\n",
    "\n",
    "        plt.imshow(contour_display_image, cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Check Tensorboard logs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd \"C:\\Users\\ansaxena\\Documents\\Python Virtual Environments\"\n",
    "deep_learning_imagenet-virtenv\\Scripts\\activate\n",
    "cd \"C:\\Users\\ansaxena\\Documents\\Projects\\Side\\VehicleDetection\\vehicle-detection-dl\\logs\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorboard --logdir=tensorboard --port=6006 --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Execute/Start Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_flag = False\n",
    "\n",
    "if grayscale_flag:\n",
    "    model_type = 'grayscale'\n",
    "else:\n",
    "    model_type = 'rgb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_filename = path_prefix + path_config.PATH_MODEL + path_config.MODEL_NAME + '_rgb_generator_224_224.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_weight_filename):\n",
    "    model_weight_filename = model_train_generator(model_type, path_prefix)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Execute/Start Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictor = ModelPredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000002.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000002.xml', 2,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000003.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000003.xml', 2,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000004.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000004.xml', 2,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000005.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000005.xml', 2,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000006.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000006.xml', 2,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000007.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000007.xml', 7,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000008.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000008.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000009.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000009.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000010.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000010.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000011.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000011.xml', 2,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000012.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000012.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000013.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000013.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000014.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000014.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000015.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000015.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000017.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000017.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "model_predictor.model_predict_rgb_xml(model_weight_filename,\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000019.jpg',\n",
    "                                      path_prefix + path_config.PATH_DATA_VALIDATION + 'validation_0000019.xml', 6,\n",
    "                                      grayscale_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vehicle-detection-dl-virtenv",
   "language": "python",
   "name": "vehicle-detection-dl-virtenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
